#!/usr/bin/env bash
#SBATCH --job-name=overall_control              # Job name
#SBATCH --ntasks=1                      # Number of MPI tasks to request
#SBATCH --cpus-per-task=30               # Number of CPU cores per MPI task
#SBATCH --mem=8G                        # Total memory to request
#SBATCH --time=0-48:00:00               # Time limit (DD-HH:MM:SS)
#SBATCH --account=chem-electro-2024        # Project account to use
#SBATCH --mail-type=END,FAIL            # Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=hll537@york.ac.uk   # Where to send mail
#SBATCH --output=slurm_logs/%x-%j.log
#SBATCH --error=slurm_logs/%x-%j.err
# Abort if any command fails

set -e
module load rclone
OUTPUT_DIR="/users/hll537/Ross_proteins/frontier_results" 
SIMULATION_DIR="/users/hll537/Ross_proteins/frontier_results/individual_simulations/"
FIRST_JOB_ID=$(sbatch --array=0-20 --export=ALL,OUTPUT_DIR=$OUTPUT_DIR synthetic_submission_2.job | awk '{print $4}')
PROCESS_JOB_ID=$(sbatch --export=ALL,OUTPUT_DIR=$OUTPUT_DIR --dependency=afterok:$FIRST_JOB_ID synthetic_get_best.job | awk '{print $4}')

#PROCESS_JOB_ID=$(sbatch --export=ALL,OUTPUT_DIR=$OUTPUT_DIR ax_get_best.job | awk '{print $4}')
FRONTPOINTS=$(python -c "import pickle; f=open('job_results.pkl','rb'); data=pickle.load(f); print(data['size'])")
KEYPOINTS=$(python -c "import pickle; f=open('job_results.pkl','rb'); data=pickle.load(f); print(data['keys'])")
TOTAL_JOBS=$(( FRONTPOINTS * KEYPOINTS -1))

# Create the array range properly
ARRAY_RANGE="0-$TOTAL_JOBS"

SIMULATION_JOB_ID=$(sbatch --dependency=afterok:$PROCESS_JOB_ID --array=$ARRAY_RANGE --export=ALL,OUTPUT_DIR=$OUTPUT_DIR synthetic_final_simulation.job | awk '{print $4}')
#SIMULATION_JOB_ID=$(sbatch --array=$ARRAY_RANGE --export=ALL,OUTPUT_DIR=$OUTPUT_DIR ax_final_simulation.job | awk '{print $4}')
python final_results_collater.py $SIMULATION_DIR "saved_simulations_"
rclone copy frontier_results gdrive:Synthetic_studies
rclone copy synthetic_data gdrive:Synthetic_studies/data
